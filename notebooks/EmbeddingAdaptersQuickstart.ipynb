{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ooutkoyXLPHL"
      },
      "source": [
        "<div align=\"center\">\n",
        "\n",
        "# \ud83e\udde0 \u2192 \ud83e\udde0 EmbeddingAdapters *Quickstart*\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/PotentiallyARobot/EmbeddingAdapters/main/embeddingadapters.png\" alt=\"EmbeddingAdapters Logo\" width=\"400\">\n",
        "\n",
        "*Bridge embedding spaces. Use adapters, not hacks.*\n",
        "\n",
        "</div>\n",
        "\n",
        "---\n",
        "\n",
        "This notebook will get you up and running with `embedding-adapters` in minutes. You'll learn how to:\n",
        "\n",
        "1. Install the library\n",
        "2. Load a pre-trained adapter\n",
        "3. Translate embeddings from a local model into a target embedding space\n",
        "4. (Optional) Score embedding quality and detect out-of-distribution inputs\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5utqJ5yLPHM"
      },
      "source": [
        "## 1. Installation\n",
        "\n",
        "Install the required packages:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dTP4Ba8qLPHN"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m0.0/41.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m41.1/41.1 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q embedding-adapters sentence-transformers torch numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JZ8J_d-LPHN"
      },
      "source": [
        "## 2. Setup & Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "TvYby0-jLPHO"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "import torch\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from embedding_adapters import EmbeddingAdapter\n",
        "\n",
        "# Detect device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtxxxNO3LPHO"
      },
      "source": [
        "### (Optional) Hugging Face Token\n",
        "\n",
        "Some adapters require a Hugging Face token. Set it here if needed:\n",
        "\n",
        "```python\n",
        "os.environ['HUGGINGFACE_TOKEN'] = 'your_token_here'\n",
        "```\n",
        "\n",
        "Or create one at: https://huggingface.co/settings/tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "VXhB8s6PLPHO"
      },
      "outputs": [],
      "source": [
        "# Uncomment and set your token if needed:\n",
        "# os.environ['HUGGINGFACE_TOKEN'] = 'hf_xxxxxxxxxxxxx'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfgucU2vLPHP"
      },
      "source": [
        "---\n",
        "\n",
        "## 3. Basic Usage: Translate Embeddings\n",
        "\n",
        "In this example, we'll:\n",
        "1. Load a **local source model** (`all-MiniLM-L6-v2`)\n",
        "2. Load a **pre-trained adapter** that maps to OpenAI's `text-embedding-3-small` space\n",
        "3. Generate embeddings and translate them into the target space"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5TOfQPrLPHP"
      },
      "source": [
        "### 3.1 Load the Source Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "X8PU0WQZLPHP"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8c64846057df4602971c984d64cc7ef5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3e50e0c7d47c4800af57546c6632b1ad"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d8f073961e484d528d6d8f91835b75cd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9b97bb04ade644c0a09cce17b28d36cf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3bba0ea659584f059cfc36408a657a57"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0cb172f54a1f454cbc2f099312b50ffa"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "925072f3a7ec4c8b80fa8ecd4072bc1a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "26208fbcf7dd4536ba17908a95215a4b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0ec88b232c75443c9b9d9a031c668bce"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9fdd9e2af05f430aaefbb018fed86795"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0105dd9525344568bd4778608d73c7f3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded source model: sentence-transformers/all-MiniLM-L6-v2\n"
          ]
        }
      ],
      "source": [
        "SOURCE_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "\n",
        "src_model = SentenceTransformer(SOURCE_MODEL, device=device)\n",
        "print(f\"Loaded source model: {SOURCE_MODEL}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4DIeOodLPHP"
      },
      "source": [
        "### 3.2 Load the Adapter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "SMdS6ViYLPHP"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[embedding_adapters] Fetching remote registry from https://raw.githubusercontent.com/PotentiallyARobot/embedding-adapters-registry/main/registry.json\n",
            "[embedding_adapters] Successfully loaded remote registry and cached at /usr/local/lib/python3.12/dist-packages/models/registry_cache/registry.json\n",
            "[embedding_adapters] Downloading adapter 'emb_adapter_minilm_to_openai_text-embedding-3-large_v1' from repo 'TylerF/emb_adapter_minilm_to_openai_text-embedding-3-large_v1' to: /usr/local/lib/python3.12/dist-packages/models/emb_adapter_minilm_to_openai_text-embedding-3-large_v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:979: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\n",
            "For more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "771ff17e83e84df2bedd7576b308869b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(\u2026)to_openai_text-embedding-3-large_v1.json:   0%|          | 0.00/193 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "be0d1aabbf2c4f35a6ca728b23c1ebbe"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "LICENSE: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1ced926639574a50b6ded1e421d52a66"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              ".gitattributes: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "98b5080a1a6e4361ab69d6e9c5d5bff1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "adapter_quality_stats_minilm_to_openai_t(\u2026):   0%|          | 0.00/48.4M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1cad2fb5231f4e8aa488de5a42403b04"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "adapter_minilm_to_openai_text-embedding-(\u2026):   0%|          | 0.00/692M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "437d6369893347e8ac193c3ae159cff1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[EmbeddingAdapter] Loaded quality stats from /usr/local/lib/python3.12/dist-packages/models/emb_adapter_minilm_to_openai_text-embedding-3-large_v1/adapter_quality_stats_minilm_to_openai_text-embedding-3-large_v1.npz\n",
            "Loaded adapter: sentence-transformers/all-MiniLM-L6-v2 \u2192 openai/text-embedding-3-small\n"
          ]
        }
      ],
      "source": [
        "TARGET_MODEL = \"openai/text-embedding-3-small\"\n",
        "\n",
        "adapter = EmbeddingAdapter.from_registry(\n",
        "    source=SOURCE_MODEL,\n",
        "    target=TARGET_MODEL,\n",
        "    flavor=\"large\",  # Options may include: \"small\", \"medium\", \"large\"\n",
        "    device=device,\n",
        "    # huggingface_token=os.environ.get('HUGGINGFACE_TOKEN')  # Uncomment if needed\n",
        ")\n",
        "\n",
        "print(f\"Loaded adapter: {SOURCE_MODEL} \u2192 {TARGET_MODEL}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4I2i1iQLPHQ"
      },
      "source": [
        "### 3.3 Encode & Translate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "4rz96UvdLPHQ"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u2705 Translated 5 embeddings in 17.76 ms\n",
            "   Average: 3.55 ms per embedding\n",
            "\n",
            "Source shape:     (5, 384)\n",
            "Translated shape: (5, 1536)\n"
          ]
        }
      ],
      "source": [
        "# Sample texts to embed\n",
        "texts = [\n",
        "    \"NASA announces discovery of Earth-like exoplanet.\",\n",
        "    \"How do I build a RAG pipeline?\",\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"Where can I find the best pizza in Washington D.C.?\",\n",
        "    \"What restaurant has deep dish pizza in Washington D.C.?\",\n",
        "]\n",
        "\n",
        "# Step 1: Generate source embeddings (normalized to match adapter training setup)\n",
        "start = time.time()\n",
        "\n",
        "src_embeddings = src_model.encode(\n",
        "    texts,\n",
        "    convert_to_numpy=True,\n",
        "    normalize_embeddings=True,  # Important: matches adapter training setup\n",
        ")\n",
        "\n",
        "# Step 2: Translate to target space\n",
        "translated_embeddings = adapter.encode_embeddings(src_embeddings)\n",
        "\n",
        "elapsed_ms = (time.time() - start) * 1000\n",
        "\n",
        "print(f\"\\n\u2705 Translated {len(texts)} embeddings in {elapsed_ms:.2f} ms\")\n",
        "print(f\"   Average: {elapsed_ms / len(texts):.2f} ms per embedding\")\n",
        "print(f\"\\nSource shape:     {src_embeddings.shape}\")\n",
        "print(f\"Translated shape: {translated_embeddings.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZF815OoWLPHQ"
      },
      "source": [
        "### 3.4 Inspect the Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "qMXOXzELLPHQ"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Text: \"NASA announces discovery of Earth-like exoplanet....\"\n",
            "  First 8 dims: [-0.02015219 -0.029815   -0.02003191  0.00570335  0.00285336  0.01528635\n",
            " -0.01628077  0.01461365]\n",
            "\n",
            "Text: \"How do I build a RAG pipeline?...\"\n",
            "  First 8 dims: [ 0.02272716  0.00379894  0.00405332 -0.03134996 -0.03544979 -0.02624673\n",
            " -0.02306672 -0.03986255]\n",
            "\n",
            "Text: \"The quick brown fox jumps over the lazy dog....\"\n",
            "  First 8 dims: [-0.02005787  0.00247312  0.02189307 -0.04284604 -0.0156842   0.01917473\n",
            "  0.02038561  0.01647771]\n",
            "\n",
            "Text: \"Where can I find the best pizza in Washington D.C....\"\n",
            "  First 8 dims: [-0.01189883 -0.02655029  0.00385452  0.03867225  0.0274068  -0.03793454\n",
            "  0.01266782  0.05449203]\n",
            "\n",
            "Text: \"Where can I find the deep dish pizza in Washington...\"\n",
            "  First 8 dims: [-0.00494743 -0.0390961  -0.00088703  0.04625957  0.0052576  -0.0256073\n",
            "  0.02638703  0.04561325]\n"
          ]
        }
      ],
      "source": [
        "# View first 8 dimensions of translated embeddings\n",
        "for i, text in enumerate(texts):\n",
        "    print(f\"\\nText: \\\"{text[:50]}...\\\"\")\n",
        "    print(f\"  First 8 dims: {translated_embeddings[i][:8]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cJMZegcLPHQ"
      },
      "source": [
        "---\n",
        "\n",
        "## 4. Compute Similarity in Target Space\n",
        "\n",
        "Translated embeddings live in the target space, so you can compute similarities just like you would with native target embeddings:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "vAHvvlUhLPHQ"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine Similarities (in target space):\n",
            "--------------------------------------------------\n",
            "[0] vs [1]: -0.0729\n",
            "    \"NASA announces discovery of Earth-like e...\"\n",
            "    \"How do I build a RAG pipeline?...\"\n",
            "\n",
            "[0] vs [2]: 0.0052\n",
            "    \"NASA announces discovery of Earth-like e...\"\n",
            "    \"The quick brown fox jumps over the lazy ...\"\n",
            "\n",
            "[0] vs [3]: 0.0356\n",
            "    \"NASA announces discovery of Earth-like e...\"\n",
            "    \"Where can I find the best pizza in Washi...\"\n",
            "\n",
            "[0] vs [4]: 0.0124\n",
            "    \"NASA announces discovery of Earth-like e...\"\n",
            "    \"What restaurant has deep dish pizza in W...\"\n",
            "\n",
            "[1] vs [2]: -0.0209\n",
            "    \"How do I build a RAG pipeline?...\"\n",
            "    \"The quick brown fox jumps over the lazy ...\"\n",
            "\n",
            "[1] vs [3]: 0.0694\n",
            "    \"How do I build a RAG pipeline?...\"\n",
            "    \"Where can I find the best pizza in Washi...\"\n",
            "\n",
            "[1] vs [4]: 0.0407\n",
            "    \"How do I build a RAG pipeline?...\"\n",
            "    \"What restaurant has deep dish pizza in W...\"\n",
            "\n",
            "[2] vs [3]: 0.0682\n",
            "    \"The quick brown fox jumps over the lazy ...\"\n",
            "    \"Where can I find the best pizza in Washi...\"\n",
            "\n",
            "[2] vs [4]: 0.0573\n",
            "    \"The quick brown fox jumps over the lazy ...\"\n",
            "    \"What restaurant has deep dish pizza in W...\"\n",
            "\n",
            "[3] vs [4]: 0.8536\n",
            "    \"Where can I find the best pizza in Washi...\"\n",
            "    \"What restaurant has deep dish pizza in W...\"\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from numpy.linalg import norm\n",
        "\n",
        "def cosine_similarity(a, b):\n",
        "    \"\"\"Compute cosine similarity between two vectors.\"\"\"\n",
        "    return np.dot(a, b) / (norm(a) * norm(b))\n",
        "\n",
        "# Compare similarity between texts\n",
        "print(\"Cosine Similarities (in target space):\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "for i in range(len(texts)):\n",
        "    for j in range(i + 1, len(texts)):\n",
        "        sim = cosine_similarity(translated_embeddings[i], translated_embeddings[j])\n",
        "        print(f\"[{i}] vs [{j}]: {sim:.4f}\")\n",
        "        print(f\"    \\\"{texts[i][:40]}...\\\"\")\n",
        "        print(f\"    \\\"{texts[j][:40]}...\\\"\")\n",
        "        print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsEBh94TLPHR"
      },
      "source": [
        "---\n",
        "\n",
        "## 5. (Optional) Quality Scoring\n",
        "\n",
        "EmbeddingAdapters can help you detect when inputs are **out-of-distribution** (OOD) for the adapter. This is useful for:\n",
        "- Debugging retrieval issues\n",
        "- Deciding when to route to a more expensive model\n",
        "- Understanding adapter limitations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "UaB8dVTTLPHR"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quality Interpretation:\n",
            "============================================================\n",
            "Adapter quality interpretation (source space)\n",
            "Number of examples: 3\n",
            "\n",
            "Example 1: 'NASA announces discovery of Earth-like exoplanet.'\n",
            "  - Confidence: 0.993  (very in-distribution)\n",
            "  - Distances: Mahalanobis = 15.205, kNN distance = 0.892\n",
            "  - Interpretation: This query looks very similar to the adapter's training data. The adapter is expected to behave reliably here.\n",
            "\n",
            "Example 2: 'How do I build a RAG pipeline?'\n",
            "  - Confidence: 0.968  (very in-distribution)\n",
            "  - Distances: Mahalanobis = 15.716, kNN distance = 1.087\n",
            "  - Interpretation: This query looks very similar to the adapter's training data. The adapter is expected to behave reliably here.\n",
            "\n",
            "Example 3: '\ud83d\ude80\ud83c\udf89\ud83d\udca1\ud83d\udd25\u2728\ud83c\udf1f\ud83c\udf8a\ud83c\udf81'\n",
            "  - Confidence: 0.000  (strongly out-of-distribution)\n",
            "  - Distances: Mahalanobis = 19.810, kNN distance = 1.112\n",
            "  - Interpretation: This query is strongly out-of-distribution. The adapter is unlikely to behave reliably for this input.\n",
            "\n",
            "Batch summary:\n",
            "  - Mean confidence: 0.654 (range: 0.000 \u2013 0.993)\n",
            "  - Many queries are borderline or out-of-distribution. Expect mixed reliability and degraded behavior on a non-trivial subset.\n"
          ]
        }
      ],
      "source": [
        "from embedding_adapters.quality import interpret_quality\n",
        "\n",
        "# Include some \"unusual\" text to see quality scoring in action\n",
        "test_texts = [\n",
        "    \"NASA announces discovery of Earth-like exoplanet.\",  # Normal\n",
        "    \"How do I build a RAG pipeline?\",                     # Normal\n",
        "    \"\ud83d\ude80\ud83c\udf89\ud83d\udca1\ud83d\udd25\u2728\ud83c\udf1f\ud83c\udf8a\ud83c\udf81\",                                        # Emojis only\n",
        "]\n",
        "\n",
        "# Generate source embeddings\n",
        "test_src_embs = src_model.encode(\n",
        "    test_texts,\n",
        "    convert_to_numpy=True,\n",
        "    normalize_embeddings=True,\n",
        ")\n",
        "\n",
        "# Score quality (uses adapter's stored quality stats)\n",
        "scores = adapter.score_source(test_src_embs)\n",
        "\n",
        "# Human-readable interpretation\n",
        "print(\"Quality Interpretation:\")\n",
        "print(\"=\" * 60)\n",
        "print(interpret_quality(test_texts, scores, space_label=\"source\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJBCR4kHLPHR"
      },
      "source": [
        "---\n",
        "\n",
        "## 6. Discover Available Adapters\n",
        "\n",
        "You can explore what adapters are available:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "RWIr__NQLPHR"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[embedding_adapters] Fetching remote registry from https://raw.githubusercontent.com/PotentiallyARobot/embedding-adapters-registry/main/registry.json\n",
            "[embedding_adapters] Successfully loaded remote registry and cached at /usr/local/lib/python3.12/dist-packages/models/registry_cache/registry.json\n",
            "[\n",
            "  {\n",
            "    \"source\": \"gemini/text-embedding-004\",\n",
            "    \"target\": \"intfloat/e5-base-v2\",\n",
            "    \"count\": 1,\n",
            "    \"adapters\": [\n",
            "      {\n",
            "        \"slug\": \"emb_adapter_gemini_text_embedding_004_to_e5-base-v2-to_linear_v1\",\n",
            "        \"flavor\": \"generic\",\n",
            "        \"pro\": true\n",
            "      }\n",
            "    ]\n",
            "  },\n",
            "  {\n",
            "    \"source\": \"gemini/text-embedding-004\",\n",
            "    \"target\": \"openai/text-embedding-3-small\",\n",
            "    \"count\": 1,\n",
            "    \"adapters\": [\n",
            "      {\n",
            "        \"slug\": \"emb_adapter_gemini-text-embedding-004_to_text-embedding-3-small_linear_v1\",\n",
            "        \"flavor\": \"generic\",\n",
            "        \"pro\": true\n",
            "      }\n",
            "    ]\n",
            "  },\n",
            "  {\n",
            "    \"source\": \"intfloat/e5-base-v2\",\n",
            "    \"target\": \"gemini/text-embedding-004\",\n",
            "    \"count\": 1,\n",
            "    \"adapters\": [\n",
            "      {\n",
            "        \"slug\": \"emb_adapter_e5-base-v2_to_gemini_text_embedding_004_small_v1\",\n",
            "        \"flavor\": \"generic\",\n",
            "        \"pro\": true\n",
            "      }\n",
            "    ]\n",
            "  },\n",
            "  {\n",
            "    \"source\": \"intfloat/e5-base-v2\",\n",
            "    \"target\": \"openai/text-embedding-3-small\",\n",
            "    \"count\": 4,\n",
            "    \"adapters\": [\n",
            "      {\n",
            "        \"slug\": \"emb_adapter_e5-base-v2-to-openai_text_embedding_3_small_v1\",\n",
            "        \"flavor\": \"small\",\n",
            "        \"pro\": false\n",
            "      },\n",
            "      {\n",
            "        \"slug\": \"emb_adapter_e5-base-v2-to-openai_text_embedding_3_large_v2\",\n",
            "        \"flavor\": \"large\",\n",
            "        \"pro\": true\n",
            "      },\n",
            "      {\n",
            "        \"slug\": \"emb_adapter_e5-base-v2-to-openai_text_embedding_3_small_v1\",\n",
            "        \"flavor\": \"linear\",\n",
            "        \"pro\": false\n",
            "      },\n",
            "      {\n",
            "        \"slug\": \"emb_adapter_e5-base-v2_to_text-embedding-3-small-v_0_1_fp16\",\n",
            "        \"flavor\": \"generic\",\n",
            "        \"pro\": true\n",
            "      }\n",
            "    ]\n",
            "  },\n",
            "  {\n",
            "    \"source\": \"openai/text-embedding-3-small\",\n",
            "    \"target\": \"gemini/text-embedding-004\",\n",
            "    \"count\": 1,\n",
            "    \"adapters\": [\n",
            "      {\n",
            "        \"slug\": \"emb_adapter_openai_text_embedding_3_small_to_gemini_text_embedding_004_linear_v1\",\n",
            "        \"flavor\": \"generic\",\n",
            "        \"pro\": true\n",
            "      }\n",
            "    ]\n",
            "  },\n",
            "  {\n",
            "    \"source\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
            "    \"target\": \"gemini/text-embedding-004\",\n",
            "    \"count\": 1,\n",
            "    \"adapters\": [\n",
            "      {\n",
            "        \"slug\": \"emb_adapter_all-MiniLM-L6-v2_to_gemini_text_embedding_004_small_v1\",\n",
            "        \"flavor\": \"generic\",\n",
            "        \"pro\": true\n",
            "      }\n",
            "    ]\n",
            "  },\n",
            "  {\n",
            "    \"source\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
            "    \"target\": \"intfloat/e5-base-v2\",\n",
            "    \"count\": 1,\n",
            "    \"adapters\": [\n",
            "      {\n",
            "        \"slug\": \"emb_adapter_minilm_L6_v2-to-e5-base-v2_large_v1\",\n",
            "        \"flavor\": \"large\",\n",
            "        \"pro\": false\n",
            "      }\n",
            "    ]\n",
            "  },\n",
            "  {\n",
            "    \"source\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
            "    \"target\": \"openai/text-embedding-3-small\",\n",
            "    \"count\": 4,\n",
            "    \"adapters\": [\n",
            "      {\n",
            "        \"slug\": \"emb_adapter_minilm_to_openai_text-embedding-3-vlarge_v1\",\n",
            "        \"flavor\": \"vlarge\",\n",
            "        \"pro\": false\n",
            "      },\n",
            "      {\n",
            "        \"slug\": \"emb_adapter_minilm_to_openai_text-embedding-3-large_v1\",\n",
            "        \"flavor\": \"large\",\n",
            "        \"pro\": false\n",
            "      },\n",
            "      {\n",
            "        \"slug\": \"emb_adapter_minilm_to_openai_text-embedding-3-medium_v1\",\n",
            "        \"flavor\": \"medium\",\n",
            "        \"pro\": false\n",
            "      },\n",
            "      {\n",
            "        \"slug\": \"emb_adapter_all-MiniLM-L6-v2_to_openai_text_embedding_3_medium\",\n",
            "        \"flavor\": \"generic\",\n",
            "        \"pro\": false\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "# List available adapters (if using CLI)\n",
        "!embedding-adapters list 2>/dev/null || echo \"Run 'embedding-adapters list' in terminal to see available adapters\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "qmVtVr88LPHR"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[embedding_adapters] Fetching remote registry from https://raw.githubusercontent.com/PotentiallyARobot/embedding-adapters-registry/main/registry.json\n",
            "[embedding_adapters] Successfully loaded remote registry and cached at /usr/local/lib/python3.12/dist-packages/models/registry_cache/registry.json\n",
            "[\n",
            "  {\n",
            "    \"source\": \"gemini/text-embedding-004\",\n",
            "    \"target\": \"intfloat/e5-base-v2\",\n",
            "    \"count\": 1,\n",
            "    \"slugs\": [\n",
            "      \"emb_adapter_gemini_text_embedding_004_to_e5-base-v2-to_linear_v1\"\n",
            "    ]\n",
            "  },\n",
            "  {\n",
            "    \"source\": \"gemini/text-embedding-004\",\n",
            "    \"target\": \"openai/text-embedding-3-small\",\n",
            "    \"count\": 1,\n",
            "    \"slugs\": [\n",
            "      \"emb_adapter_gemini-text-embedding-004_to_text-embedding-3-small_linear_v1\"\n",
            "    ]\n",
            "  },\n",
            "  {\n",
            "    \"source\": \"intfloat/e5-base-v2\",\n",
            "    \"target\": \"gemini/text-embedding-004\",\n",
            "    \"count\": 1,\n",
            "    \"slugs\": [\n",
            "      \"emb_adapter_e5-base-v2_to_gemini_text_embedding_004_small_v1\"\n",
            "    ]\n",
            "  },\n",
            "  {\n",
            "    \"source\": \"intfloat/e5-base-v2\",\n",
            "    \"target\": \"openai/text-embedding-3-small\",\n",
            "    \"count\": 4,\n",
            "    \"slugs\": [\n",
            "      \"emb_adapter_e5-base-v2-to-openai_text_embedding_3_small_v1\",\n",
            "      \"emb_adapter_e5-base-v2-to-openai_text_embedding_3_large_v2\",\n",
            "      \"emb_adapter_e5-base-v2-to-openai_text_embedding_3_small_v1\",\n",
            "      \"emb_adapter_e5-base-v2_to_text-embedding-3-small-v_0_1_fp16\"\n",
            "    ]\n",
            "  },\n",
            "  {\n",
            "    \"source\": \"openai/text-embedding-3-small\",\n",
            "    \"target\": \"gemini/text-embedding-004\",\n",
            "    \"count\": 1,\n",
            "    \"slugs\": [\n",
            "      \"emb_adapter_openai_text_embedding_3_small_to_gemini_text_embedding_004_linear_v1\"\n",
            "    ]\n",
            "  },\n",
            "  {\n",
            "    \"source\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
            "    \"target\": \"gemini/text-embedding-004\",\n",
            "    \"count\": 1,\n",
            "    \"slugs\": [\n",
            "      \"emb_adapter_all-MiniLM-L6-v2_to_gemini_text_embedding_004_small_v1\"\n",
            "    ]\n",
            "  },\n",
            "  {\n",
            "    \"source\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
            "    \"target\": \"intfloat/e5-base-v2\",\n",
            "    \"count\": 1,\n",
            "    \"slugs\": [\n",
            "      \"emb_adapter_minilm_L6_v2-to-e5-base-v2_large_v1\"\n",
            "    ]\n",
            "  },\n",
            "  {\n",
            "    \"source\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
            "    \"target\": \"openai/text-embedding-3-small\",\n",
            "    \"count\": 4,\n",
            "    \"slugs\": [\n",
            "      \"emb_adapter_minilm_to_openai_text-embedding-3-vlarge_v1\",\n",
            "      \"emb_adapter_minilm_to_openai_text-embedding-3-large_v1\",\n",
            "      \"emb_adapter_minilm_to_openai_text-embedding-3-medium_v1\",\n",
            "      \"emb_adapter_all-MiniLM-L6-v2_to_openai_text_embedding_3_medium\"\n",
            "    ]\n",
            "  }\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "# Show available source \u2192 target paths\n",
        "!embedding-adapters paths 2>/dev/null || echo \"Run 'embedding-adapters paths' in terminal to see available paths\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hy0sW1sLPHR"
      },
      "source": [
        "---\n",
        "\n",
        "## 7. Example: Query an Existing Index\n",
        "\n",
        "A common use case is querying a corpus that was embedded with an expensive cloud model, using a cheap local model + adapter:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "fkWqO_TvLPHR"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corpus: 5 documents embedded\n"
          ]
        }
      ],
      "source": [
        "# Simulated corpus (in production, this would be your vector DB)\n",
        "corpus = [\n",
        "    \"Python is a popular programming language for data science.\",\n",
        "    \"Machine learning models require large amounts of training data.\",\n",
        "    \"Vector databases store embeddings for semantic search.\",\n",
        "    \"RAG combines retrieval with language model generation.\",\n",
        "    \"The capital of France is Paris.\",\n",
        "]\n",
        "\n",
        "# Simulate: corpus was embedded with the target model (OpenAI)\n",
        "# In reality, you'd have these stored in your vector DB\n",
        "corpus_embeddings = src_model.encode(\n",
        "    corpus,\n",
        "    convert_to_numpy=True,\n",
        "    normalize_embeddings=True,\n",
        ")\n",
        "corpus_translated = adapter.encode_embeddings(corpus_embeddings)\n",
        "\n",
        "print(f\"Corpus: {len(corpus)} documents embedded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "3Sj3WAxiLPHR"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: \"What should I use to store vectors for search?\"\n",
            "\n",
            "Top results:\n",
            "  1. [0.6497] Vector databases store embeddings for semantic search.\n",
            "  2. [0.3361] RAG combines retrieval with language model generation.\n",
            "  3. [0.2169] Machine learning models require large amounts of training data.\n"
          ]
        }
      ],
      "source": [
        "def search(query: str, top_k: int = 3):\n",
        "    \"\"\"Search the corpus using adapter-translated query embedding.\"\"\"\n",
        "    # Embed query with local model\n",
        "    query_emb = src_model.encode(\n",
        "        [query],\n",
        "        convert_to_numpy=True,\n",
        "        normalize_embeddings=True,\n",
        "    )\n",
        "\n",
        "    # Translate to target space\n",
        "    query_translated = adapter.encode_embeddings(query_emb)[0]\n",
        "\n",
        "    # Compute similarities\n",
        "    similarities = [\n",
        "        cosine_similarity(query_translated, doc_emb)\n",
        "        for doc_emb in corpus_translated\n",
        "    ]\n",
        "\n",
        "    # Rank results\n",
        "    ranked = sorted(enumerate(similarities), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    return [(corpus[idx], score) for idx, score in ranked[:top_k]]\n",
        "\n",
        "\n",
        "# Try a query\n",
        "query = \"What should I use to store vectors for search?\"\n",
        "results = search(query)\n",
        "\n",
        "print(f\"Query: \\\"{query}\\\"\\n\")\n",
        "print(\"Top results:\")\n",
        "for i, (doc, score) in enumerate(results, 1):\n",
        "    print(f\"  {i}. [{score:.4f}] {doc}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsZssN2JLPHR"
      },
      "source": [
        "---\n",
        "\n",
        "## 8. Summary \ud83e\udde0 \u2192 \ud83e\udde0\n",
        "\n",
        "You've learned how to:\n",
        "\n",
        "| Task | Code |\n",
        "|------|------|\n",
        "| Load a source model | `SentenceTransformer(model_name)` |\n",
        "| Load an adapter | `EmbeddingAdapter.from_registry(source, target, flavor)` |\n",
        "| Generate embeddings | `src_model.encode(texts, normalize_embeddings=True)` |\n",
        "| Translate embeddings | `adapter.encode_embeddings(src_embs)` |\n",
        "| Score quality | `adapter.score_source(src_embs)` |\n",
        "\n",
        "### Key Points\n",
        "\n",
        "- **Always normalize** source embeddings (`normalize_embeddings=True`) to match adapter training\n",
        "- Translated embeddings are compatible with your target embedding space\n",
        "- Quality scoring helps detect out-of-distribution inputs\n",
        "- Adapters run locally \u2014 no API calls, no latency, no cost per embedding\n",
        "\n",
        "---\n",
        "\n",
        "## Next Steps\n",
        "\n",
        "- \ud83d\udcda [Full Documentation](https://github.com/PotentiallyARobot/EmbeddingAdapters)\n",
        "- \ud83d\udd0d Explore available adapters: `embedding-adapters list`\n",
        "- \ud83e\uddea Evaluate on your own data before production use\n",
        "- \ud83d\udcac Open an issue or PR for bugs, ideas, or new adapter requests"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}